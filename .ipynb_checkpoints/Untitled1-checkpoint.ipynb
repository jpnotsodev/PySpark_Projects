{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8d338e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                  Version\n",
      "------------------------ -----------\n",
      "alembic                  1.7.7\n",
      "anyio                    3.6.2\n",
      "argon2-cffi              21.3.0\n",
      "argon2-cffi-bindings     21.2.0\n",
      "arrow                    1.2.3\n",
      "asgiref                  3.6.0\n",
      "asttokens                2.0.8\n",
      "async-generator          1.10\n",
      "attrs                    21.4.0\n",
      "backcall                 0.2.0\n",
      "beautifulsoup4           4.11.1\n",
      "bidict                   0.21.4\n",
      "bleach                   6.0.0\n",
      "blinker                  1.4\n",
      "bs4                      0.0.1\n",
      "cachetools               5.2.0\n",
      "certifi                  2021.10.8\n",
      "cffi                     1.15.0\n",
      "charset-normalizer       2.0.11\n",
      "click                    8.0.4\n",
      "colorama                 0.4.4\n",
      "cryptography             37.0.2\n",
      "debugpy                  1.6.3\n",
      "decorator                5.1.1\n",
      "defusedxml               0.7.1\n",
      "distlib                  0.3.4\n",
      "Django                   4.1.5\n",
      "dnspython                2.2.1\n",
      "email-validator          1.1.3\n",
      "entrypoints              0.4\n",
      "et-xmlfile               1.1.0\n",
      "eventlet                 0.33.0\n",
      "executing                1.0.0\n",
      "fastjsonschema           2.16.3\n",
      "filelock                 3.6.0\n",
      "Flask                    2.0.3\n",
      "Flask-Login              0.5.0\n",
      "Flask-Mail               0.9.1\n",
      "Flask-Migrate            3.1.0\n",
      "Flask-SocketIO           5.1.1\n",
      "Flask-SQLAlchemy         2.5.1\n",
      "Flask-WTF                1.0.0\n",
      "fqdn                     1.5.1\n",
      "google-api-core          2.8.2\n",
      "google-api-python-client 2.51.0\n",
      "google-auth              2.8.0\n",
      "google-auth-httplib2     0.1.0\n",
      "googleapis-common-protos 1.56.2\n",
      "greenlet                 1.1.2\n",
      "h11                      0.13.0\n",
      "httplib2                 0.20.4\n",
      "idna                     3.3\n",
      "ipykernel                6.15.2\n",
      "ipython                  8.4.0\n",
      "ipython-genutils         0.2.0\n",
      "ipython-sql              0.5.0\n",
      "isoduration              20.11.0\n",
      "itsdangerous             2.1.1\n",
      "jedi                     0.18.1\n",
      "Jinja2                   3.0.3\n",
      "jsonpointer              2.3\n",
      "jsonschema               4.17.3\n",
      "jupyter_client           8.2.0\n",
      "jupyter_core             5.3.0\n",
      "jupyter-events           0.6.3\n",
      "jupyter_server           2.5.0\n",
      "jupyter_server_terminals 0.4.4\n",
      "jupyterlab-pygments      0.2.2\n",
      "Mako                     1.2.0\n",
      "MarkupSafe               2.1.1\n",
      "matplotlib-inline        0.1.6\n",
      "mistune                  2.0.5\n",
      "nbclassic                0.5.5\n",
      "nbclient                 0.7.3\n",
      "nbconvert                7.3.1\n",
      "nbformat                 5.8.0\n",
      "nest-asyncio             1.5.5\n",
      "notebook                 6.5.4\n",
      "notebook_shim            0.2.2\n",
      "numpy                    1.22.3\n",
      "openpyxl                 3.0.10\n",
      "outcome                  1.2.0\n",
      "packaging                21.3\n",
      "pandas                   1.4.1\n",
      "pandocfilters            1.5.0\n",
      "parso                    0.8.3\n",
      "pickleshare              0.7.5\n",
      "pip                      22.1.2\n",
      "platformdirs             2.5.1\n",
      "prettytable              3.3.0\n",
      "progress                 1.6\n",
      "prometheus-client        0.16.0\n",
      "prompt-toolkit           3.0.30\n",
      "protobuf                 3.20.1\n",
      "psutil                   5.9.1\n",
      "psycopg2                 2.9.6\n",
      "pure-eval                0.2.2\n",
      "py4j                     0.10.9.7\n",
      "pyasn1                   0.4.8\n",
      "pyasn1-modules           0.2.8\n",
      "pycparser                2.21\n",
      "Pygments                 2.13.0\n",
      "PyMySQL                  1.0.2\n",
      "pyocclient               0.6\n",
      "pyodbc                   4.0.32\n",
      "pyOpenSSL                22.0.0\n",
      "pyparsing                3.0.9\n",
      "pyrfc                    3.3\n",
      "pyrsistent               0.19.3\n",
      "PySocks                  1.7.1\n",
      "pyspark                  3.4.0\n",
      "python-dateutil          2.8.2\n",
      "python-dotenv            1.0.0\n",
      "python-engineio          4.3.1\n",
      "python-json-logger       2.0.7\n",
      "python-socketio          5.5.2\n",
      "pytube                   12.1.0\n",
      "pytz                     2022.1\n",
      "pywinpty                 2.0.10\n",
      "PyYAML                   6.0\n",
      "pyzmq                    25.0.2\n",
      "requests                 2.27.1\n",
      "rfc3339-validator        0.1.4\n",
      "rfc3986-validator        0.1.1\n",
      "rsa                      4.8\n",
      "sap-gui-auto             0.1\n",
      "schedule                 1.2.0\n",
      "scheduler                0.8.4\n",
      "selenium                 4.2.0\n",
      "Send2Trash               1.8.0\n",
      "setuptools               57.4.0\n",
      "six                      1.16.0\n",
      "sniffio                  1.2.0\n",
      "sortedcontainers         2.4.0\n",
      "soupsieve                2.3.2.post1\n",
      "SQLAlchemy               2.0.11\n",
      "sqlparse                 0.4.3\n",
      "stack-data               0.5.0\n",
      "terminado                0.17.1\n",
      "tinycss2                 1.2.1\n",
      "tornado                  6.2\n",
      "tqdm                     4.65.0\n",
      "traitlets                5.9.0\n",
      "trio                     0.21.0\n",
      "trio-websocket           0.9.2\n",
      "typeguard                4.0.0\n",
      "typing_extensions        4.5.0\n",
      "tzdata                   2022.7\n",
      "uri-template             1.2.0\n",
      "uritemplate              4.1.1\n",
      "urllib3                  1.26.8\n",
      "virtualenv               20.13.2\n",
      "wcwidth                  0.2.5\n",
      "webcolors                1.13\n",
      "webdriver-manager        3.8.6\n",
      "webencodings             0.5.1\n",
      "websocket-client         1.5.1\n",
      "Werkzeug                 2.0.3\n",
      "wsproto                  1.1.0\n",
      "WTForms                  3.0.1\n",
      "YouTube-Downloader       1.0\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51262326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32c365dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('test').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72f18db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Builder',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_activeSession',\n",
       " '_convert_from_pandas',\n",
       " '_createFromLocal',\n",
       " '_createFromRDD',\n",
       " '_create_dataframe',\n",
       " '_create_from_pandas_with_arrow',\n",
       " '_create_shell_session',\n",
       " '_getActiveSessionOrCreate',\n",
       " '_get_numpy_record_dtype',\n",
       " '_inferSchema',\n",
       " '_inferSchemaFromList',\n",
       " '_instantiatedSession',\n",
       " '_jconf',\n",
       " '_jsc',\n",
       " '_jsparkSession',\n",
       " '_jvm',\n",
       " '_repr_html_',\n",
       " '_sc',\n",
       " 'builder',\n",
       " 'catalog',\n",
       " 'conf',\n",
       " 'createDataFrame',\n",
       " 'getActiveSession',\n",
       " 'newSession',\n",
       " 'range',\n",
       " 'read',\n",
       " 'readStream',\n",
       " 'sparkContext',\n",
       " 'sql',\n",
       " 'stop',\n",
       " 'streams',\n",
       " 'table',\n",
       " 'udf',\n",
       " 'version']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e33effb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DataFrameReader in module pyspark.sql.readwriter object:\n",
      "\n",
      "class DataFrameReader(OptionUtils)\n",
      " |  DataFrameReader(spark: 'SparkSession')\n",
      " |  \n",
      " |  Interface used to load a :class:`DataFrame` from external storage systems\n",
      " |  (e.g. file systems, key-value stores, etc). Use :attr:`SparkSession.read`\n",
      " |  to access this.\n",
      " |  \n",
      " |  .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  .. versionchanged:: 3.4.0\n",
      " |      Supports Spark Connect.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DataFrameReader\n",
      " |      OptionUtils\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, spark: 'SparkSession')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  csv(self, path: Union[str, List[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, sep: Optional[str] = None, encoding: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, comment: Optional[str] = None, header: Union[bool, str, NoneType] = None, inferSchema: Union[bool, str, NoneType] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, nanValue: Optional[str] = None, positiveInf: Optional[str] = None, negativeInf: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, maxColumns: Union[int, str, NoneType] = None, maxCharsPerColumn: Union[int, str, NoneType] = None, maxMalformedLogPerPartition: Union[int, str, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, enforceSchema: Union[bool, str, NoneType] = None, emptyValue: Optional[str] = None, locale: Optional[str] = None, lineSep: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, unescapedQuoteHandling: Optional[str] = None) -> 'DataFrame'\n",
      " |      Loads a CSV file and returns the result as a  :class:`DataFrame`.\n",
      " |      \n",
      " |      This function will go through the input once to determine the input schema if\n",
      " |      ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n",
      " |      ``inferSchema`` option or specify the schema explicitly using ``schema``.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str or list\n",
      " |          string, or list of strings, for input path(s),\n",
      " |          or RDD of Strings storing CSV rows.\n",
      " |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
      " |          an optional :class:`pyspark.sql.types.StructType` for the input schema\n",
      " |          or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      Extra options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n",
      " |          for the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Write a DataFrame into a CSV file and read it back.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a CSV file\n",
      " |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
      " |      ...     df.write.mode(\"overwrite\").format(\"csv\").save(d)\n",
      " |      ...\n",
      " |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n",
      " |      ...     spark.read.csv(d, schema=df.schema, nullValue=\"Hyukjin Kwon\").show()\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      |100|null|\n",
      " |      +---+----+\n",
      " |  \n",
      " |  format(self, source: str) -> 'DataFrameReader'\n",
      " |      Specifies the input data source format.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      source : str\n",
      " |          string, name of the data source, e.g. 'json', 'parquet'.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> spark.read.format('json')\n",
      " |      <...readwriter.DataFrameReader object ...>\n",
      " |      \n",
      " |      Write a DataFrame into a JSON file and read it back.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a JSON file\n",
      " |      ...     spark.createDataFrame(\n",
      " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      " |      ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\n",
      " |      ...\n",
      " |      ...     # Read the JSON file as a DataFrame.\n",
      " |      ...     spark.read.format('json').load(d).show()\n",
      " |      +---+------------+\n",
      " |      |age|        name|\n",
      " |      +---+------------+\n",
      " |      |100|Hyukjin Kwon|\n",
      " |      +---+------------+\n",
      " |  \n",
      " |  jdbc(self, url: str, table: str, column: Optional[str] = None, lowerBound: Union[int, str, NoneType] = None, upperBound: Union[int, str, NoneType] = None, numPartitions: Optional[int] = None, predicates: Optional[List[str]] = None, properties: Optional[Dict[str, str]] = None) -> 'DataFrame'\n",
      " |      Construct a :class:`DataFrame` representing the database table named ``table``\n",
      " |      accessible via JDBC URL ``url`` and connection ``properties``.\n",
      " |      \n",
      " |      Partitions of the table will be retrieved in parallel if either ``column`` or\n",
      " |      ``predicates`` is specified. ``lowerBound``, ``upperBound`` and ``numPartitions``\n",
      " |      is needed when ``column`` is specified.\n",
      " |      \n",
      " |      If both ``column`` and ``predicates`` are specified, ``column`` will be used.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      table : str\n",
      " |          the name of the table\n",
      " |      column : str, optional\n",
      " |          alias of ``partitionColumn`` option. Refer to ``partitionColumn`` in\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n",
      " |          for the version you use.\n",
      " |      predicates : list, optional\n",
      " |          a list of expressions suitable for inclusion in WHERE clauses;\n",
      " |          each one defines one partition of the :class:`DataFrame`\n",
      " |      properties : dict, optional\n",
      " |          a dictionary of JDBC database connection arguments. Normally at\n",
      " |          least properties \"user\" and \"password\" with their corresponding values.\n",
      " |          For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      Extra options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n",
      " |          for the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Don't create too many partitions in parallel on a large cluster;\n",
      " |      otherwise Spark might crash your external database systems.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |  \n",
      " |  json(self, path: Union[str, List[str], pyspark.rdd.RDD[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, primitivesAsString: Union[bool, str, NoneType] = None, prefersDecimal: Union[bool, str, NoneType] = None, allowComments: Union[bool, str, NoneType] = None, allowUnquotedFieldNames: Union[bool, str, NoneType] = None, allowSingleQuotes: Union[bool, str, NoneType] = None, allowNumericLeadingZero: Union[bool, str, NoneType] = None, allowBackslashEscapingAnyCharacter: Union[bool, str, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, allowUnquotedControlChars: Union[bool, str, NoneType] = None, lineSep: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, dropFieldIfAllNull: Union[bool, str, NoneType] = None, encoding: Optional[str] = None, locale: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, allowNonNumericNumbers: Union[bool, str, NoneType] = None) -> 'DataFrame'\n",
      " |      Loads JSON files and returns the results as a :class:`DataFrame`.\n",
      " |      \n",
      " |      `JSON Lines <http://jsonlines.org/>`_ (newline-delimited JSON) is supported by default.\n",
      " |      For JSON (one record per file), set the ``multiLine`` parameter to ``true``.\n",
      " |      \n",
      " |      If the ``schema`` parameter is not specified, this function goes\n",
      " |      through the input once to determine the input schema.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str, list or :class:`RDD`\n",
      " |          string represents path to the JSON dataset, or a list of paths,\n",
      " |          or RDD of Strings storing JSON objects.\n",
      " |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
      " |          an optional :class:`pyspark.sql.types.StructType` for the input schema or\n",
      " |          a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      Extra options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n",
      " |          for the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Write a DataFrame into a JSON file and read it back.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a JSON file\n",
      " |      ...     spark.createDataFrame(\n",
      " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      " |      ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\n",
      " |      ...\n",
      " |      ...     # Read the JSON file as a DataFrame.\n",
      " |      ...     spark.read.json(d).show()\n",
      " |      +---+------------+\n",
      " |      |age|        name|\n",
      " |      +---+------------+\n",
      " |      |100|Hyukjin Kwon|\n",
      " |      +---+------------+\n",
      " |  \n",
      " |  load(self, path: Union[str, List[str], NoneType] = None, format: Optional[str] = None, schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, **options: 'OptionalPrimitiveType') -> 'DataFrame'\n",
      " |      Loads data from a data source and returns it as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str or list, optional\n",
      " |          optional string or a list of string for file-system backed data sources.\n",
      " |      format : str, optional\n",
      " |          optional string for format of the data source. Default to 'parquet'.\n",
      " |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
      " |          optional :class:`pyspark.sql.types.StructType` for the input schema\n",
      " |          or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      " |      **options : dict\n",
      " |          all other string options\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Load a CSV file with format, schema and options specified.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a CSV file with a header\n",
      " |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
      " |      ...     df.write.option(\"header\", True).mode(\"overwrite\").format(\"csv\").save(d)\n",
      " |      ...\n",
      " |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon',\n",
      " |      ...     # and 'header' option set to `True`.\n",
      " |      ...     df = spark.read.load(\n",
      " |      ...         d, schema=df.schema, format=\"csv\", nullValue=\"Hyukjin Kwon\", header=True)\n",
      " |      ...     df.printSchema()\n",
      " |      ...     df.show()\n",
      " |      root\n",
      " |       |-- age: long (nullable = true)\n",
      " |       |-- name: string (nullable = true)\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      |100|null|\n",
      " |      +---+----+\n",
      " |  \n",
      " |  option(self, key: str, value: 'OptionalPrimitiveType') -> 'DataFrameReader'\n",
      " |      Adds an input option for the underlying data source.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : str\n",
      " |          The key for the option to set.\n",
      " |      value\n",
      " |          The value for the option to set.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> spark.read.option(\"key\", \"value\")\n",
      " |      <...readwriter.DataFrameReader object ...>\n",
      " |      \n",
      " |      Specify the option 'nullValue' with reading a CSV file.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a CSV file\n",
      " |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
      " |      ...     df.write.mode(\"overwrite\").format(\"csv\").save(d)\n",
      " |      ...\n",
      " |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n",
      " |      ...     spark.read.schema(df.schema).option(\n",
      " |      ...         \"nullValue\", \"Hyukjin Kwon\").format('csv').load(d).show()\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      |100|null|\n",
      " |      +---+----+\n",
      " |  \n",
      " |  options(self, **options: 'OptionalPrimitiveType') -> 'DataFrameReader'\n",
      " |      Adds input options for the underlying data source.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **options : dict\n",
      " |          The dictionary of string keys and prmitive-type values.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> spark.read.option(\"key\", \"value\")\n",
      " |      <...readwriter.DataFrameReader object ...>\n",
      " |      \n",
      " |      Specify the option 'nullValue' and 'header' with reading a CSV file.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a CSV file with a header.\n",
      " |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
      " |      ...     df.write.option(\"header\", True).mode(\"overwrite\").format(\"csv\").save(d)\n",
      " |      ...\n",
      " |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon',\n",
      " |      ...     # and 'header' option set to `True`.\n",
      " |      ...     spark.read.options(\n",
      " |      ...         nullValue=\"Hyukjin Kwon\",\n",
      " |      ...         header=True\n",
      " |      ...     ).format('csv').load(d).show()\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      |100|null|\n",
      " |      +---+----+\n",
      " |  \n",
      " |  orc(self, path: Union[str, List[str]], mergeSchema: Optional[bool] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None) -> 'DataFrame'\n",
      " |      Loads ORC files, returning the result as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str or list\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      Extra options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-orc.html#data-source-option>`_\n",
      " |          for the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Write a DataFrame into a ORC file and read it back.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a ORC file\n",
      " |      ...     spark.createDataFrame(\n",
      " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      " |      ...     ).write.mode(\"overwrite\").format(\"orc\").save(d)\n",
      " |      ...\n",
      " |      ...     # Read the Parquet file as a DataFrame.\n",
      " |      ...     spark.read.orc(d).show()\n",
      " |      +---+------------+\n",
      " |      |age|        name|\n",
      " |      +---+------------+\n",
      " |      |100|Hyukjin Kwon|\n",
      " |      +---+------------+\n",
      " |  \n",
      " |  parquet(self, *paths: str, **options: 'OptionalPrimitiveType') -> 'DataFrame'\n",
      " |      Loads Parquet files, returning the result as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      paths : str\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#data-source-option>`_\n",
      " |          for the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Write a DataFrame into a Parquet file and read it back.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a Parquet file\n",
      " |      ...     spark.createDataFrame(\n",
      " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      " |      ...     ).write.mode(\"overwrite\").format(\"parquet\").save(d)\n",
      " |      ...\n",
      " |      ...     # Read the Parquet file as a DataFrame.\n",
      " |      ...     spark.read.parquet(d).show()\n",
      " |      +---+------------+\n",
      " |      |age|        name|\n",
      " |      +---+------------+\n",
      " |      |100|Hyukjin Kwon|\n",
      " |      +---+------------+\n",
      " |  \n",
      " |  schema(self, schema: Union[pyspark.sql.types.StructType, str]) -> 'DataFrameReader'\n",
      " |      Specifies the input schema.\n",
      " |      \n",
      " |      Some data sources (e.g. JSON) can infer the input schema automatically from data.\n",
      " |      By specifying the schema here, the underlying data source can skip the schema\n",
      " |      inference step, and thus speed up data loading.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      schema : :class:`pyspark.sql.types.StructType` or str\n",
      " |          a :class:`pyspark.sql.types.StructType` object or a DDL-formatted string\n",
      " |          (For example ``col0 INT, col1 DOUBLE``).\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> spark.read.schema(\"col0 INT, col1 DOUBLE\")\n",
      " |      <...readwriter.DataFrameReader object ...>\n",
      " |      \n",
      " |      Specify the schema with reading a CSV file.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     spark.read.schema(\"col0 INT, col1 DOUBLE\").format(\"csv\").load(d).printSchema()\n",
      " |      root\n",
      " |       |-- col0: integer (nullable = true)\n",
      " |       |-- col1: double (nullable = true)\n",
      " |  \n",
      " |  table(self, tableName: str) -> 'DataFrame'\n",
      " |      Returns the specified table as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      tableName : str\n",
      " |          string, name of the table.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.range(10)\n",
      " |      >>> df.createOrReplaceTempView('tblA')\n",
      " |      >>> spark.read.table('tblA').show()\n",
      " |      +---+\n",
      " |      | id|\n",
      " |      +---+\n",
      " |      |  0|\n",
      " |      |  1|\n",
      " |      |  2|\n",
      " |      |  3|\n",
      " |      |  4|\n",
      " |      |  5|\n",
      " |      |  6|\n",
      " |      |  7|\n",
      " |      |  8|\n",
      " |      |  9|\n",
      " |      +---+\n",
      " |      >>> _ = spark.sql(\"DROP TABLE tblA\")\n",
      " |  \n",
      " |  text(self, paths: Union[str, List[str]], wholetext: bool = False, lineSep: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None) -> 'DataFrame'\n",
      " |      Loads text files and returns a :class:`DataFrame` whose schema starts with a\n",
      " |      string column named \"value\", and followed by partitioned columns if there\n",
      " |      are any.\n",
      " |      The text files must be encoded as UTF-8.\n",
      " |      \n",
      " |      By default, each line in the text file is a new row in the resulting DataFrame.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      paths : str or list\n",
      " |          string, or list of strings, for input path(s).\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      Extra options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-text.html#data-source-option>`_\n",
      " |          for the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Write a DataFrame into a text file and read it back.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a text file\n",
      " |      ...     df = spark.createDataFrame([(\"a\",), (\"b\",), (\"c\",)], schema=[\"alphabets\"])\n",
      " |      ...     df.write.mode(\"overwrite\").format(\"text\").save(d)\n",
      " |      ...\n",
      " |      ...     # Read the text file as a DataFrame.\n",
      " |      ...     spark.read.schema(df.schema).text(d).sort(\"alphabets\").show()\n",
      " |      +---------+\n",
      " |      |alphabets|\n",
      " |      +---------+\n",
      " |      |        a|\n",
      " |      |        b|\n",
      " |      |        c|\n",
      " |      +---------+\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from OptionUtils:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88c05677",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv').load('D:\\R123.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1afa4e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+-----+----+------------------+-------------+-----+-------+--------+\n",
      "|               _c0|                 _c1|  _c2| _c3|               _c4|          _c5|  _c6|    _c7|     _c8|\n",
      "+------------------+--------------------+-----+----+------------------+-------------+-----+-------+--------+\n",
      "|            EposNo|           TransDate|Plant|Sloc|             MatNo|         ISBN|  Qty|  Price|Discount|\n",
      "|EPOS70750000112675|10/13/2023 12:00:...| RB00|R123|000000008000048594|8888822006194|1.000| 848.00|   10.00|\n",
      "|EPOS70750000112676|10/13/2023 12:00:...| RB00|R123|000000008000049694|9789814995047|1.000|1195.00|   10.00|\n",
      "|EPOS70750000112677|10/13/2023 12:00:...| RB00|R123|000000008000049694|9789814995047|1.000|1195.00|   10.00|\n",
      "|EPOS70750000112678|10/13/2023 12:00:...| RB00|R123|000000008000048407|8888822005388|1.000|3398.00|   10.00|\n",
      "|EPOS70750000112679|10/13/2023 12:00:...| RB00|R123|000000008000026123|9789719810674|1.000| 345.00|    5.00|\n",
      "|EPOS70750000112680|10/13/2023 12:00:...| RB00|R123|000000008000043158|9789719813224|1.000| 598.00|   10.00|\n",
      "|EPOS70750000112680|10/13/2023 12:00:...| RB00|R123|000000008000043545|9789814865036|1.000|1795.00|   10.00|\n",
      "|EPOS70750000112680|10/13/2023 12:00:...| RB00|R123|000000008000045994|9781265348441|1.000|2595.00|   20.00|\n",
      "|EPOS70750000112681|10/13/2023 12:00:...| RB00|R123|000000008000048407|8888822005388|1.000|3398.00|   10.00|\n",
      "|EPOS70750000112682|10/13/2023 12:00:...| RB00|R123|000000008000026123|9789719810674|1.000| 345.00|    5.00|\n",
      "|EPOS70750000112683|10/13/2023 12:00:...| RB00|R123|000000008000026123|9789719810674|1.000| 345.00|    5.00|\n",
      "|EPOS70750000112684|10/13/2023 12:00:...| RB00|R123|000000008000049694|9789814995047|1.000|1195.00|   10.00|\n",
      "|EPOS70750000112685|10/13/2023 12:00:...| RB00|R123|000000008000026123|9789719810674|1.000| 345.00|    5.00|\n",
      "|EPOS70750000112685|10/13/2023 12:00:...| RB00|R123|000000008000045994|9781265348441|1.000|2595.00|   20.00|\n",
      "|EPOS70750000112685|10/13/2023 12:00:...| RB00|R123|000000008000049694|9789814995047|1.000|1195.00|   10.00|\n",
      "|EPOS70750000112686|10/13/2023 12:00:...| RB00|R123|000000008000049694|9789814995047|1.000|1195.00|   10.00|\n",
      "|EPOS70750000112687|10/13/2023 12:00:...| RB00|R123|000000008000026123|9789719810674|1.000| 345.00|    5.00|\n",
      "|EPOS70750000112688|10/13/2023 12:00:...| RB00|R123|000000008000026123|9789719810674|1.000| 345.00|    5.00|\n",
      "|EPOS70750000112688|10/13/2023 12:00:...| RB00|R123|000000008000045994|9781265348441|1.000|2595.00|   20.00|\n",
      "+------------------+--------------------+-----+----+------------------+-------------+-----+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9686337",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
